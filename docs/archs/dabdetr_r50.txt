DABDETR(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (head): DABDETRHead(
    (projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (pos_encoding): SineEncoding(temperature=20)
    (encoder): DABDETREncoder(
      (layers): ModuleList(
        (0): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.0)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.0)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.00909090880304575)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.00909090880304575)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.0181818176060915)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.0181818176060915)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.027272727340459824)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.027272727340459824)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.036363635212183)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.036363635212183)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DABDETREncoderLayer(
          (attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.045454543083906174)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.045454543083906174)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DABDETRDecoder(
      (layers): ModuleList(
        (0): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.054545458406209946)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.054545458406209946)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.054545458406209946)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.06363636255264282)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.06363636255264282)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.06363636255264282)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.0727272778749466)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.0727272778749466)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.0727272778749466)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.08181818574666977)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.08181818574666977)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.08181818574666977)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.09090909361839294)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.09090909361839294)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.09090909361839294)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DABDETRDecoderLayer(
          (self_attn): MultiheadAttention(
            (in_proj_q): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop1): DropPath(drop_prob=0.10000000149011612)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MultiheadAttentionV2(
            (in_proj_q_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_q_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_c): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_k_p): Linear(in_features=256, out_features=256, bias=True)
            (in_proj_v): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop2): DropPath(drop_prob=0.10000000149011612)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): PReLU(num_parameters=1)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop3): DropPath(drop_prob=0.10000000149011612)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (pos_encoding): SineEncoding(temperature=10000)
      (mlp_pe_proj): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (mlp_ref_xy): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (mlp_ref_wh): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=2, bias=True)
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (reg_top): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (cls_top): Linear(in_features=256, out_features=20, bias=True)
    )
  )
  (reg_loss): SmoothL1Loss()
  (iou_loss): GIoULoss()
  (cls_loss): FocalLoss()
  (postprocess): MultiLabelBasicProcess(min_score=0.01)
)